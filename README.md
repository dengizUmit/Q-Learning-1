# Q-Learning
The model-free reinforcement learning techniques as the Q-learning algorithm can be successful with relatively small training episodes in learning how to navigate independently, it converged 80% of the time after only 25,000 training episodes. The high complexity of the algorithm may restrict the training capacity as it took over 11 hours to train, while in execution it took less than 4 seconds on average for each episode. The success rate was also affected by the fact that the episode terminates right after colliding with an obstacle, rather than continuing the navigation afterwards, to ensure proper obstacle avoiding approach to be returned in the Q-table. Other design strategies as continuous navigation after collision may increase the accuracy significantly. Thereâ€™s a great potential for the off-policy Q-learning algorithm in fire-extinguishing applications, more powerful computing machines can enable training over a higher amount of episodes to check the highest possible accuracy before stagnation.
